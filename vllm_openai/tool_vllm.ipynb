{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NousResearch/Hermes-2-Pro-Llama-3-8B\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set up this example by starting a vLLM OpenAI-compatible server with tool call\n",
    "options enabled. For example:\n",
    "\n",
    "IMPORTANT: for mistral, you must use one of the provided mistral tool call\n",
    "templates, or your own - the model default doesn't work for tool calls with vLLM\n",
    "See the vLLM docs on OpenAI server & tool calling for more details.\n",
    "\n",
    "vllm serve --model mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "            --chat-template examples/tool_chat_template_mistral.jinja \\\n",
    "            --enable-auto-tool-choice --tool-call-parser mistral\n",
    "\n",
    "OR\n",
    "vllm serve --model NousResearch/Hermes-2-Pro-Llama-3-8B \\\n",
    "            --chat-template examples/tool_chat_template_hermes.jinja \\\n",
    "            --enable-auto-tool-choice --tool-call-parser hermes\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"The city to find the weather for, e.g. 'San Francisco'\"\n",
    "                },\n",
    "                \"state\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"the two-letter abbreviation for the state that the city is\"\n",
    "                    \" in, e.g. 'CA' which would mean 'California'\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit to fetch the temperature in\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\", \"state\", \"unit\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hi! How are you doing today?\"\n",
    "}, {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"I'm doing well! How can I help you?\"\n",
    "}, {\n",
    "    \"role\":\n",
    "    \"user\",\n",
    "    \"content\":\n",
    "    \"Can you tell me what the temperate will be in Dallas, in fahrenheit?\"\n",
    "}]\n",
    "\n",
    "chat_completion = client.chat.completions.create(messages=messages,\n",
    "                                                 model=model,\n",
    "                                                 tools=tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion results:\n",
      "ChatCompletion(id='chat-cfe697ea7cd8419ca61c12566192b06e', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-8e07be64ff0643fd94a11fa324352a80', function=Function(arguments='{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), type='function')]), stop_reason=None)], created=1726492700, model='NousResearch/Hermes-2-Pro-Llama-3-8B', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=34, prompt_tokens=464, total_tokens=498), prompt_logprobs=None)\n",
      "\n",
      "\n",
      "\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None)\n",
      "ChoiceDeltaToolCall(index=0, id='chatcmpl-tool-4c4a7b56b0e84dd4b657645b218b7da0', function=ChoiceDeltaToolCallFunction(arguments=None, name='get_current_weather'), type='function')\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"city\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Dallas', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\", \"state\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='TX', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\", \"unit\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='f', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='ahrenheit', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "streamed tool call id: chatcmpl-tool-4c4a7b56b0e84dd4b657645b218b7da0 \n",
      "streamed tool call name: get_current_weather\n",
      "streamed tool call arguments: {\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Chat completion results:\")\n",
    "print(chat_completion)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "tool_calls_stream = client.chat.completions.create(messages=messages,\n",
    "                                                   model=model,\n",
    "                                                   tools=tools,\n",
    "                                                   stream=True)\n",
    "\n",
    "chunks = []\n",
    "for chunk in tool_calls_stream:\n",
    "    chunks.append(chunk)\n",
    "    if chunk.choices[0].delta.tool_calls:\n",
    "        print(chunk.choices[0].delta.tool_calls[0])\n",
    "    else:\n",
    "        print(chunk.choices[0].delta)\n",
    "\n",
    "arguments = []\n",
    "tool_call_idx = -1\n",
    "for chunk in chunks:\n",
    "\n",
    "    if chunk.choices[0].delta.tool_calls:\n",
    "        tool_call = chunk.choices[0].delta.tool_calls[0]\n",
    "\n",
    "        if tool_call.index != tool_call_idx:\n",
    "            if tool_call_idx >= 0:\n",
    "                print(\n",
    "                    f\"streamed tool call arguments: {arguments[tool_call_idx]}\"\n",
    "                )\n",
    "            tool_call_idx = chunk.choices[0].delta.tool_calls[0].index\n",
    "            arguments.append(\"\")\n",
    "        if tool_call.id:\n",
    "            print(f\"streamed tool call id: {tool_call.id} \")\n",
    "\n",
    "        if tool_call.function:\n",
    "            if tool_call.function.name:\n",
    "                print(f\"streamed tool call name: {tool_call.function.name}\")\n",
    "\n",
    "            if tool_call.function.arguments:\n",
    "                arguments[tool_call_idx] += tool_call.function.arguments\n",
    "\n",
    "if len(arguments):\n",
    "    print(f\"streamed tool call arguments: {arguments[-1]}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"tool_calls\": chat_completion.choices[0].message.tool_calls\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Dallas, Texas is 85 degrees fahrenheit. It is partly cloudly, with highs in the 90's.\n",
      "\n",
      "\n",
      "\n",
      "ChatCompletion(id='chat-17a005e3c407470bbb3391db5bc196e0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current temperature in Dallas, Texas is 85 degrees Fahrenheit. It is partly cloudy, with highs in the 90s.', refusal=None, role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1726492725, model='NousResearch/Hermes-2-Pro-Llama-3-8B', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=27, prompt_tokens=536, total_tokens=563), prompt_logprobs=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, simulate a tool call\n",
    "def get_current_weather(city: str, state: str, unit: 'str'):\n",
    "    return (\"The weather in Dallas, Texas is 85 degrees fahrenheit. It is \"\n",
    "            \"partly cloudly, with highs in the 90's.\")\n",
    "\n",
    "\n",
    "available_tools = {\"get_current_weather\": get_current_weather}\n",
    "\n",
    "completion_tool_calls = chat_completion.choices[0].message.tool_calls\n",
    "for call in completion_tool_calls:\n",
    "    tool_to_call = available_tools[call.function.name]\n",
    "    args = json.loads(call.function.arguments)\n",
    "    result = tool_to_call(**args)\n",
    "    print(result)\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": result,\n",
    "        \"tool_call_id\": call.id,\n",
    "        \"name\": call.function.name\n",
    "    })\n",
    "\n",
    "chat_completion_2 = client.chat.completions.create(messages=messages,\n",
    "                                                   model=model,\n",
    "                                                   tools=tools,\n",
    "                                                   stream=False)\n",
    "print(\"\\n\\n\")\n",
    "print(chat_completion_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbqr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
