{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 14:32:14 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='NousResearch/Hermes-2-Pro-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Pro-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Pro-Llama-3-8B, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 09-25 14:32:16 model_runner.py:997] Starting to load model NousResearch/Hermes-2-Pro-Llama-3-8B...\n",
      "INFO 09-25 14:32:17 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:10<00:32, 10.87s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:22<00:23, 11.58s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:26<00:07,  7.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:38<00:00,  9.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:38<00:00,  9.56s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 14:32:56 model_runner.py:1008] Loading model weights took 14.9605 GB\n",
      "INFO 09-25 14:32:57 gpu_executor.py:122] # GPU blocks: 13277, # CPU blocks: 2048\n",
      "INFO 09-25 14:33:00 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-25 14:33:00 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-25 14:33:19 model_runner.py:1430] Graph capturing finished in 19 secs.\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "from outlines import generate, models\n",
    "\n",
    "llm = vllm.LLM(\"NousResearch/Hermes-2-Pro-Llama-3-8B\")\n",
    "model = models.VLLM(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "\n",
    "def wikipedia(q):\n",
    "    return httpx.get(\"https://en.wikipedia.org/w/api.php\", params={\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": q,\n",
    "        \"format\": \"json\"\n",
    "    }).json()[\"query\"][\"search\"][0][\"snippet\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mean(nums: list[int]) -> float:\n",
    "    return pd.Series(nums).mean()\n",
    "\n",
    "def avg(nums: list[int]) -> float:\n",
    "    return pd.Series(nums).mean()\n",
    "\n",
    "def get_max(nums: list[int]) -> int:\n",
    "    return pd.Series(nums).max()\n",
    "\n",
    "def get_min(nums: list[int]) -> int:\n",
    "    return pd.Series(nums).min()\n",
    "\n",
    "def get_sum(nums: list[int]) -> int:\n",
    "    return pd.Series(nums).sum()\n",
    "\n",
    "def sort(nums: list[int]) -> list[int]:\n",
    "    return list(pd.Series(nums).sort_values())\n",
    "\n",
    "\n",
    "class Action(str, Enum):\n",
    "    mean = \"mean\"\n",
    "    avg = \"avg\"\n",
    "    get_max = \"get_max\"\n",
    "    get_min = \"get_min\"\n",
    "    get_sum = \"get_sum\"\n",
    "    sort = \"sort\"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class Reason_and_Act(BaseModel):\n",
    "    Scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n",
    "    Thought: str = Field(..., description=\"It describes your thoughts about the question you have been asked\")\n",
    "    Action: Action\n",
    "    Action_Input: str = Field(..., description=\"The arguments of the Action.\")\n",
    "    \n",
    "class Final_Answer(BaseModel):\n",
    "    Scratchpad: str = Field(..., description=\"Information from the Observation useful to answer the question\")\n",
    "    Final_Answer: str = Field(..., description=\"Answer to the question grounded on the Observation\")\n",
    "    \n",
    "\n",
    "class Decision(BaseModel):\n",
    "    Decision: Union[Reason_and_Act, Final_Answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\{[ ]?\"Decision\"[ ]?:[ ]?(\\{[ ]?\"Scratchpad\"[ ]?:[ ]?\"([^\"\\\\\\x00-\\x1F\\x7F-\\x9F]|\\\\[\"\\\\])*\"[ ]?,[ ]?\"Thought\"[ ]?:[ ]?\"([^\"\\\\\\x00-\\x1F\\x7F-\\x9F]|\\\\[\"\\\\])*\"[ ]?,[ ]?\"Action\"[ ]?:[ ]?(\"mean\"|\"avg\"|\"get_max\"|\"get_min\"|\"get_sum\"|\"sort\")[ ]?,[ ]?\"Action_Input\"[ ]?:[ ]?\"([^\"\\\\\\x00-\\x1F\\x7F-\\x9F]|\\\\[\"\\\\])*\"[ ]?\\}|\\{[ ]?\"Scratchpad\"[ ]?:[ ]?\"([^\"\\\\\\x00-\\x1F\\x7F-\\x9F]|\\\\[\"\\\\])*\"[ ]?,[ ]?\"Final_Answer\"[ ]?:[ ]?\"([^\"\\\\\\x00-\\x1F\\x7F-\\x9F]|\\\\[\"\\\\])*\"[ ]?\\})[ ]?\\}\n"
     ]
    }
   ],
   "source": [
    "from outlines.integrations.utils import convert_json_schema_to_str\n",
    "from outlines.fsm.json_schema import build_regex_from_schema\n",
    "\n",
    "json_schema = Decision.model_json_schema()\n",
    "schema_str = convert_json_schema_to_str(json_schema=json_schema)\n",
    "regex_str = build_regex_from_schema(schema_str)\n",
    "print(regex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def generate_hermes_prompt(question, schema=\"\"):\n",
    "    return (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a world class AI model who answers questions in JSON with correct Pydantic schema. \"\n",
    "        f\"Here's the json schema you must adhere to:\\n<schema>\\n{schema}\\n</schema>\\n\"\n",
    "        \"Today is \" + datetime.datetime.today().strftime('%Y-%m-%d') + \".\\n\" +\n",
    "        \"You run in a loop of Scratchpad, Thought, Action, Action Input, PAUSE, Observation. \"\n",
    "        \"At the end of the loop you output a Final Answer. \"\n",
    "        \"Use Scratchpad to store the information from the Observation useful to answer the question \"\n",
    "        \"Use Thought to describe your thoughts about the question you have been asked \"\n",
    "        \"and reflect carefully about the Observation if it exists. \"\n",
    "        \"Use Action to run one of the actions available to you. \"\n",
    "        \"Use Action Input to input the arguments of the selected action - then return PAUSE. \"\n",
    "        \"Observation will be the result of running those actions. \"\n",
    "        \"Your available actions are:\\n\"\n",
    "        \"mean:\\n\"\n",
    "        \"e.g. mean: [1, 2, 3, 4, 5]\\n\"\n",
    "        \"Calculates the mean (average) of a list of numbers.\\n\"\n",
    "        \"avg:\\n\"\n",
    "        \"e.g. avg: [1, 2, 3, 4, 5]\\n\"\n",
    "        \"Alias for mean - calculates the average of a list of numbers.\\n\"\n",
    "        \"get_max:\\n\"\n",
    "        \"e.g. get_max: [1, 2, 3, 4, 5]\\n\"\n",
    "        \"Finds the maximum value in a list of numbers.\\n\"\n",
    "        \"get_min:\\n\"\n",
    "        \"e.g. get_min: [1, 2, 3, 4, 5]\\n\"\n",
    "        \"Finds the minimum value in a list of numbers.\\n\"\n",
    "        \"get_sum:\\n\"\n",
    "        \"e.g. get_sum: [1, 2, 3, 4, 5]\\n\"\n",
    "        \"Calculates the sum of a list of numbers.\\n\"\n",
    "        \"sort:\\n\"\n",
    "        \"e.g. sort: [5, 1, 3, 2, 4]\\n\"\n",
    "        \"Sorts a list of numbers in ascending order.\\n\"\n",
    "        \"Think step by step and always use minimum one action. \"\n",
    "        \"DO NOT TRY TO GUESS THE ANSWER. Begin! <|im_end|>\"\n",
    "        \"\\n<|im_start|>user\\n\" + question + \"<|im_end|>\"\n",
    "        \"\\n<|im_start|>assistant\\n\"    \n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self, prompt=\"\"):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def __call__(self, user_prompt):\n",
    "        self.prompt += user_prompt\n",
    "        result = self.execute()\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        generator = generate.regex(model, regex_str)\n",
    "        result = generator(self.prompt, max_tokens=1024, seed=42)\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    next_prompt = (\n",
    "        \"\\n<|im_start|>user\\n\" + question + \"<|im_end|>\"\n",
    "        \"\\n<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    previous_actions = []\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        prompt = generate_hermes_prompt(question=question, schema=Decision.model_json_schema())\n",
    "        bot = ChatBot(prompt=prompt)\n",
    "        result = bot(next_prompt)\n",
    "        json_result = json.loads(result)['Decision']\n",
    "        if \"Final_Answer\" not in list(json_result.keys()):\n",
    "            scratchpad = json_result['Scratchpad'] if i == 0 else \"\"\n",
    "            thought = json_result['Thought']\n",
    "            action = json_result['Action']\n",
    "            action_input = json_result['Action_Input']\n",
    "            print(f\"\\x1b[34m Scratchpad: {scratchpad} \\x1b[0m\")\n",
    "            print(f\"\\x1b[34m Thought: {thought} \\x1b[0m\")\n",
    "            print(f\"\\x1b[36m  -- running {action}: {str(action_input)}\\x1b[0m\")\n",
    "            if action + \": \" + str(action_input) in previous_actions:\n",
    "                observation = \"You already ran that action. **TRY A DIFFERENT ACTION INPUT.**\"\n",
    "            else:\n",
    "                try:\n",
    "                    # Check if action is in the defined enum and perform the corresponding action\n",
    "                    if action in Action.__members__:\n",
    "                        # Convert the action string to the corresponding function call\n",
    "                        observation = globals()[action](action_input)\n",
    "                    else:\n",
    "                        observation = \"Invalid action.\"\n",
    "                except Exception as e:\n",
    "                    observation = f\"{e}\"\n",
    "            print()\n",
    "            print(f\"\\x1b[33m Observation: {observation} \\x1b[0m\")\n",
    "            print()\n",
    "            previous_actions.append(action + \": \" + str(action_input))\n",
    "            next_prompt += (\n",
    "                \"\\nScratchpad: \" + scratchpad +\n",
    "                \"\\nThought: \" + thought +\n",
    "                \"\\nAction: \" + action  +\n",
    "                \"\\nAction Input: \" + action_input +\n",
    "                \"\\nObservation: \" + str(observation)\n",
    "            )\n",
    "        else:\n",
    "            scratchpad = json_result[\"Scratchpad\"]\n",
    "            final_answer = json_result[\"Final_Answer\"]\n",
    "            print(f\"\\x1b[34m Scratchpad: {scratchpad} \\x1b[0m\")\n",
    "            print(f\"\\x1b[34m Final Answer: {final_answer} \\x1b[0m\")\n",
    "            return final_answer\n",
    "    print(\"\\nFinal Answer: I am sorry, but I am unable to answer your question. Please provide more information or a different question.\")\n",
    "    return \"No answer found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it, est. speed input: 685.73 toks/s, output: 36.77 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m Scratchpad: Caterpillar's average total revenue and Realogy's lowest net income from 2019 to 2021 need to be calculated and compared. \u001b[0m\n",
      "\u001b[34m Final Answer: Caterpillar's average total revenue was higher than Realogy's lowest net income from 2019 to 2021. \u001b[0m\n",
      "Caterpillar's average total revenue was higher than Realogy's lowest net income from 2019 to 2021.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(query(\"Think step by step and reason about your answer. Was Caterpillar's average total revenue higher or lower than Realogy's lowest net income from 2019 to 2021? Here is the table as markdown: '|    | company1               | facts              |   year |      value |\\n|---:|:-----------------------|:-------------------|-------:|-----------:|\\n|  0 | CATERPILLAR INC        | us-gaap:Revenues   |   2019 | 5.38e+10   |\\n|  1 | Realogy Holdings Corp. | us-gaap:ProfitLoss |   2019 | 1.85e+08   |\\n|  2 | CATERPILLAR INC        | us-gaap:Revenues   |   2020 | 4.1748e+10 |\\n|  3 | Realogy Holdings Corp. | us-gaap:ProfitLoss |   2020 | 3.56e+08   |\\n|  4 | CATERPILLAR INC        | us-gaap:Revenues   |   2021 | 5.0971e+10 |\\n|  5 | Realogy Holdings Corp. | us-gaap:ProfitLoss |   2021 | 3.5e+08    |'\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbqr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
